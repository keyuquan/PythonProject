#!/usr/bin/python
# encoding: utf-8

"""
决策树
一.建树的重要概念
1.树：根节点,叶子节点,非叶子节点和分支
2.熵：对数据混乱程度的描述,数据越混乱,熵值越大；对已经发生的某个事件（互斥事件）,发生的概率为 p1,p2...pn 则 数据的熵 =p1*log2(p1) + p2*log2(p2) + .. + pn*log2(pn)
3.信息增益 ： 某个事件的发生的熵 - 在某种不同情况下 该事件发生的熵  例如： 出去打球的熵- 在天气不同情况下,出去打球的熵
4.信息增益率 : 某种不同情况下 某种事件发生的熵 的变化情况；  防止数据太过细化,或者不相干因素的影响；
            例如： 对 id ; 对于每个 id,出去打球的概率都为1或0,熵 为 0； 熵的变化也为0,即是 信息增益率为 0
5.GINI系数：和熵的衡量标准类似,计算方式不相同

二.决策数建立的优化
6.决策树剪枝策略
    预剪枝：限制深度,叶子节点个数,叶子节点样本数,信息增益率 等
    后剪枝：通过一定的衡量标准
7.随机森林： 就是通过随机选择数据多次建树 形成多棵树,避免极特殊数据的影响

三.决策数参数讲解
 def __init__(self,
                 criterion="gini",
                 splitter="best",
                 max_depth=None,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 min_weight_fraction_leaf=0.,
                 max_features=None,
                 random_state=None,
                 max_leaf_nodes=None,
                 min_impurity_decrease=0.,
                 min_impurity_split=None,
                 class_weight=None,
                 presort=False):

1.criterion  gini  or  entropy

2.splitter  best or random 前者是在所有特征中找最好的切分点 后者是在部分特征中（数据量大的时候）

3.max_features  None（所有）,log2,sqrt,N  特征小于50的时候一般使用所有的

4.max_depth  数据少或者特征少的时候可以不管这个值,如果模型样本量多,特征也多的情况下,可以尝试限制下

5.min_samples_split  如果某节点的样本数少于min_samples_split,则不会继续再尝试选择最优特征来进行划分
                     如果样本量不大,不需要管这个值。如果样本量数量级非常大,则推荐增大这个值。

6.min_samples_leaf  这个值限制了叶子节点最少的样本数,如果某叶子节点数目小于样本数,则会和兄弟节点一起被
                    剪枝,如果样本量不大,不需要管这个值,大些如10W可是尝试下5

7.min_weight_fraction_leaf 这个值限制了叶子节点所有样本权重和的最小值,如果小于这个值,则会和兄弟节点一起
                        被剪枝默认是0,就是不考虑权重问题。一般来说,如果我们有较多样本有缺失值,
                        或者分类树样本的分布类别偏差很大,就会引入样本权重,这时我们就要注意这个值了。

8.max_leaf_nodes 通过限制最大叶子节点数,可以防止过拟合,默认是"None”,即不限制最大的叶子节点数。
                 如果加了限制,算法会建立在最大叶子节点数内最优的决策树。
                 如果特征不多,可以不考虑这个值,但是如果特征分成多的话,可以加以限制
                 具体的值可以通过交叉验证得到。

9.class_weight 指定样本各类别的的权重,主要是为了防止训练集某些类别的样本过多
               导致训练的决策树过于偏向这些类别。这里可以自己指定各个样本的权重
               如果使用“balanced”,则算法会自己计算权重,样本量少的类别所对应的样本权重会高。

10.min_impurity_split 这个值限制了决策树的增长,如果某节点的不纯度
                     (基尼系数,信息增益,均方差,绝对差)小于这个阈值
                     则该节点不再生成子节点。即为叶子节点 。

"""
